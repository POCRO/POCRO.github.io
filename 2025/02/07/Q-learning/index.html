<!DOCTYPE html>
<html lang="zh-Han">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="POCRO">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://pocro.github.io/2025/02/07/q-learning/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="参考博客与论文Q-learning  Q-Learning Algorithms: A Comprehensive Classification and Applications  1.基本概念Q-learning 的核心是学习一个 Q 函数，该函数估计了在给定状态（State）下，采取某个动作（Action）所能获得的长期奖励的期望值。它的目标是找到一个最优策略，使得智能体在每个状态下都能选择最">
<meta property="og:type" content="article">
<meta property="og:title" content="Q-learning">
<meta property="og:url" content="https://pocro.github.io/2025/02/07/Q-learning/index.html">
<meta property="og:site_name" content="POCRO的个人站">
<meta property="og:description" content="参考博客与论文Q-learning  Q-Learning Algorithms: A Comprehensive Classification and Applications  1.基本概念Q-learning 的核心是学习一个 Q 函数，该函数估计了在给定状态（State）下，采取某个动作（Action）所能获得的长期奖励的期望值。它的目标是找到一个最优策略，使得智能体在每个状态下都能选择最">
<meta property="og:locale">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071001810.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071003103.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071004942.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071006683.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071006459.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071007589.png">
<meta property="article:published_time" content="2025-02-07T01:46:33.000Z">
<meta property="article:modified_time" content="2025-02-07T02:07:13.833Z">
<meta property="article:author" content="Flying Pocro">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071001810.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            Q-learning -
        
        POCRO的个人站
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"pocro.github.io","root":"/","language":"zh-Han","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"POCRO的个人站","subtitle":{"text":["Welcome!"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.4.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":null,"Me":"/about","Github":"https://github.com/POCRO"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2023/8/29"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="https://pocro.github.io/">
                
                POCRO的个人站
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        ABOUT
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                ABOUT
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular">Q-learning</h1>
            
            </div>
            
                    
        
        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/redefine-avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">POCRO</span>
                        
                            <span class="author-label">Lv2</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-02-07 09:46:33</span>
        <span class="mobile">2025-02-07 09:46:33</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-02-07 10:07:13</span>
            <span class="mobile">2025-02-07 10:07:13</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <h3 id="参考博客与论文"><a href="#参考博客与论文" class="headerlink" title="参考博客与论文"></a>参考博客与论文</h3><p><a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Q-learning" >Q-learning <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8836506" >Q-Learning Algorithms: A Comprehensive Classification and Applications <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><p>Q-learning 的核心是学习一个 <strong>Q 函数</strong>，该函数估计了在给定状态（State）下，采取某个动作（Action）所能获得的长期奖励的期望值。它的目标是找到一个最优策略，使得智能体在每个状态下都能选择最优动作，从而最大化累积的奖励。</p>
<p>Q 函数的定义如下：</p>
<p>Q(s,a)&#x3D;当前状态 s 下，选择动作 a 所获得的长期回报</p>
<p>其中：</p>
<ul>
<li><p>s是状态空间中的一个状态。</p>
</li>
<li><p>a是智能体在状态 s 下可以选择的动作。</p>
</li>
<li><p>Q(s,a)是智能体在状态 s 下采取动作 a 后，能够获得的期望奖励。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071001810.png"
                      alt="image-20250207100142778"
                ></p>
</li>
</ul>
<p>其中<em>Rt+1</em>是从 state  St移动到 state  St+1时收到的奖励，α是<a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Learning_rate" >学习率 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>(0&lt;α≤1)。</p>
<ol>
<li><p>学习率α</p>
<p>决定了新获取的信息在多大程度上覆盖旧信息。</p>
<p>因子 0 使代理什么都学不到（专门利用先验知识），而因子 1 使代理只考虑最新的信息（忽略先验知识以探索可能性）。</p>
<p>在完全<a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deterministic_system" >确定性 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>环境中，学习率 αt&#x3D;1 是最优的。</p>
<p>当问题是<a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stochastic_systems" >随机 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的时，算法在某些技术条件下收敛于学习率，这要求它减少到零。在实践中，通常使用恒定的学习率，例如  αt&#x3D;0.1</p>
</li>
<li><p>衰减系数γ</p>
<p>决定了未来奖励的重要性。</p>
<p>系数为 0 将使代理仅考虑当前奖励（即rt）（在上面的更新规则中）而变得“短视”，而接近 1 的系数将使其争取长期的高奖励。如果折扣系数达到或超过 1，则作值可能会有所不同。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071003103.png"
                      alt="γ 越大，agent对未来越了解，反之γ越小，agent越“短视” ，对未来状态的预测减弱"
                ></p>
<p>γ 越大，agent对未来越了解，反之γ越小，agent越“短视” ，对未来状态的预测减弱</p>
</li>
<li><p>epsilon探索策略</p>
<p>用于权衡探索&#x2F;利用（exploration&#x2F;exploitation）</p>
<p>探索：执行随机的动作</p>
<p>利用：</p>
<p>引用自：<a class="link"   target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2018-04-17-3" >https://www.jiqizhixin.com/articles/2018-04-17-3 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>一般情况下刚开始的Q值都默认为0, 也就是Q-TABLE中的任何值都不知道，所以需要通过随机选择动作进行大量的探索。</p>
<p>生成一个随机数，如果这个数大于epsilon，那么就会“利用”Q值进行更新（利用已知知识信息探索动作），否则我们将继续进行随机探索。</p>
<p>可见：epsilon 越大，越倾向于随机探索;  epsilon 越小，越倾向于利用已得知识</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071004942.png"
                      alt="image-20250207100415899"
                ></p>
</li>
<li><p>确定条件下的Q-Learning的收敛性分析</p>
<p>可以证明，<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/365814943" >https://zhuanlan.zhihu.com/p/365814943 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 任意的状态动作对任意的状态动作对（s,a）,其中s∈A,a∈A，Q^(s,a)将收敛到 Q*(s,a)。其中 Q<em>就是最优策略  π</em>所对应的Q</p>
</li>
</ol>
<h2 id="2-关于On-policy和Off-policy"><a href="#2-关于On-policy和Off-policy" class="headerlink" title="2. 关于On-policy和Off-policy"></a>2. 关于On-policy和Off-policy</h2><p>在强化学习中，<strong>On-policy</strong> 和 <strong>Off-policy</strong> 主要的区别在于智能体学习的过程中，<strong>策略的更新</strong>和<strong>行为的选择</strong>是否依赖于同一策略。</p>
<ul>
<li><strong>On-policy</strong> 就像是一个学生<strong>只能根据自己当前的学习方法</strong>来学习并改进自己，所采取的学习策略直接影响自己的学习过程。</li>
<li><strong>Off-policy</strong> 就像是一个学生在学习时可以<strong>参考别人（老师或其他人的策略）</strong>，即使自己并没有按照这个策略进行学习，也能从其他人的行为中获得反馈来改进自己的策略。</li>
</ul>
<h3 id="1-On-policy（策略学习与行为选择一致）："><a href="#1-On-policy（策略学习与行为选择一致）：" class="headerlink" title="1. On-policy（策略学习与行为选择一致）："></a>1. <strong>On-policy</strong>（策略学习与行为选择一致）：</h3><p>在 On-policy 学习中，智能体在学习过程中，<strong>使用同一个策略</strong>来选择动作，并且<strong>基于这个策略的行为来更新自己的策略</strong>。也就是说，智能体的行为和它学习的策略是紧密相关的。这个策略不仅用于决策，还被用来指导更新 Q 值。</p>
<h3 id="类比："><a href="#类比：" class="headerlink" title="类比："></a>类比：</h3><p>假设你正在学习开车，你决定遵循某个驾驶技巧（比如“慢速启动，平稳加速”），你按照这个方法开车并通过经验调整你的驾驶技巧。你的驾驶技巧（策略）和你实际开车时采取的动作（行为）是相同的，你根据实际开车的表现来逐步改进技巧。</p>
<h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><ul>
<li><strong>SARSA</strong>（State-Action-Reward-State-Action）是 On-policy 算法。在 SARSA 中，智能体在每个状态下选择一个动作，然后根据该动作的奖励和下一个状态，继续根据当前策略进行选择。策略的更新是基于智能体自己采取的动作的。</li>
</ul>
<h3 id="2-Off-policy（策略学习与行为选择不一致）："><a href="#2-Off-policy（策略学习与行为选择不一致）：" class="headerlink" title="2. Off-policy（策略学习与行为选择不一致）："></a>2. <strong>Off-policy</strong>（策略学习与行为选择不一致）：</h3><p>在 Off-policy 学习中，智能体<strong>可以根据一个策略</strong>来学习，但<strong>它不必按照这个策略来选择动作</strong>，也就是说，智能体可能会用不同的策略来选择动作，并且在学习时依赖于另一个（行为）策略的经验来更新它的学习策略。</p>
<h3 id="类比：-1"><a href="#类比：-1" class="headerlink" title="类比："></a>类比：</h3><p>现在你还是在学习开车，但是你决定向有经验的老司机请教，尽管你自己还没有完全学会这个技巧。你观察老司机是如何驾驶的，模拟他们的操作（比如“快速启动，紧急刹车”），然后自己尝试通过这些观察来改进你的驾驶技巧。这里，你学习的技巧和你实际开车时采用的方式可以不同。</p>
<h3 id="例子：-1"><a href="#例子：-1" class="headerlink" title="例子："></a>例子：</h3><ul>
<li><strong>Q-learning</strong> 是 Off-policy 算法。在 Q-learning 中，智能体选择一个动作来与环境交互，并根据环境的反馈来更新 Q 值。虽然它使用 <strong>ε-贪婪策略</strong>（通过随机选择动作来探索环境），但它更新 Q 值时是基于“最优策略”（即选择最大 Q 值的动作）来学习的，不依赖于实际采取的动作。</li>
</ul>
<h3 id="Q-learning-是-Off-policy："><a href="#Q-learning-是-Off-policy：" class="headerlink" title="Q-learning 是 Off-policy："></a>Q-learning 是 <strong>Off-policy</strong>：</h3><p>Q-learning 是一种典型的 <strong>Off-policy</strong> 算法。它通过更新 Q 函数来找到最优策略，但它在选择动作时并不完全依赖于当前的 Q 函数，而是采用了 <strong>ε-贪婪策略</strong>，即有时选择随机动作（探索），有时选择 Q 值最大的动作（利用）。因此，Q-learning 中的学习与行为选择是<strong>不一致的</strong>，也就是 <strong>Off-policy</strong>。</p>
<h2 id="3-Q-Learning-demo-for-Blackjack-in-Gymnasium（二十一点）"><a href="#3-Q-Learning-demo-for-Blackjack-in-Gymnasium（二十一点）" class="headerlink" title="3. Q-Learning demo for Blackjack in Gymnasium（二十一点）"></a>3. Q-Learning demo for Blackjack in Gymnasium（二十一点）</h2><p><a class="link"   target="_blank" rel="noopener" href="https://gymnasium.org.cn/introduction/train_agent/" >https://gymnasium.org.cn/introduction/train_agent/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<ol>
<li><p>二十一点规则</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://gymnasium.org.cn/environments/toy_text/blackjack/" >https://gymnasium.org.cn/environments/toy_text/blackjack/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</li>
<li><p>具体实现</p>
<ol>
<li>训练代码</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> BlackjackAgent <span class="keyword">import</span> BlackjackAgent  <span class="comment"># 导入 BlackjackAgent 类</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">n_episodes = <span class="number">100_000</span></span><br><span class="line">start_epsilon = <span class="number">1.0</span></span><br><span class="line">epsilon_decay = start_epsilon / (n_episodes / <span class="number">2</span>)  <span class="comment"># 逐步减少探索</span></span><br><span class="line">final_epsilon = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建环境</span></span><br><span class="line">env = gym.make(<span class="string">&quot;Blackjack-v1&quot;</span>, sab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化代理</span></span><br><span class="line">agent = BlackjackAgent(</span><br><span class="line">    env=env,</span><br><span class="line">    learning_rate=learning_rate,</span><br><span class="line">    initial_epsilon=start_epsilon,</span><br><span class="line">    epsilon_decay=epsilon_decay,</span><br><span class="line">    final_epsilon=final_epsilon,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">env = gym.wrappers.RecordEpisodeStatistics(env)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于记录奖励和误差</span></span><br><span class="line">episode_rewards = []</span><br><span class="line">episode_errors = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_episodes)):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        action = agent.get_action(obs)</span><br><span class="line">        next_obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新代理</span></span><br><span class="line">        agent.update(obs, action, reward, terminated, next_obs)</span><br><span class="line"></span><br><span class="line">        total_reward += reward</span><br><span class="line">        done = terminated <span class="keyword">or</span> truncated</span><br><span class="line">        obs = next_obs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 记录每回合的奖励和误差</span></span><br><span class="line">    episode_rewards.append(total_reward)</span><br><span class="line">    episode_errors.append(np.mean(agent.training_error))  <span class="comment"># 记录当前回合的平均误差</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新 epsilon</span></span><br><span class="line">    agent.decay_epsilon()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练完成，绘制奖励和误差曲线</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制奖励曲线</span></span><br><span class="line">ax1.plot(np.convolve(episode_rewards, np.ones(<span class="number">100</span>)/<span class="number">100</span>, mode=<span class="string">&#x27;valid&#x27;</span>), label=<span class="string">&quot;Total Reward&quot;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&quot;Total Reward Over Episodes&quot;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&quot;Reward&quot;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 TD 误差曲线</span></span><br><span class="line">ax2.plot(episode_errors, label=<span class="string">&quot;TD Error&quot;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&quot;TD Error Over Episodes&quot;</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&quot;TD Error&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================== 方法二：测试智能体性能 ================== #</span></span><br><span class="line">test_episodes = <span class="number">1000</span>  <span class="comment"># 运行 1000 回合</span></span><br><span class="line">total_wins = <span class="number">0</span></span><br><span class="line">total_games = <span class="number">0</span></span><br><span class="line">total_rewards = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(test_episodes):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        action = np.argmax(agent.q_values[obs])  <span class="comment"># 选择最优策略</span></span><br><span class="line">        obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line">        total_reward += reward</span><br><span class="line">        done = terminated <span class="keyword">or</span> truncated</span><br><span class="line"></span><br><span class="line">    total_rewards.append(total_reward)</span><br><span class="line">    <span class="keyword">if</span> total_reward &gt; <span class="number">0</span>:</span><br><span class="line">        total_wins += <span class="number">1</span></span><br><span class="line">    total_games += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">win_rate = total_wins / total_games * <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Win rate over <span class="subst">&#123;test_episodes&#125;</span> episodes: <span class="subst">&#123;win_rate:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制测试奖励分布直方图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.hist(total_rewards, bins=<span class="number">20</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Total Reward&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Frequency&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test Reward Distribution&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div>

<ol start="2">
<li>agent类代码</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BlackjackAgent.py</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BlackjackAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        env: gym.Env,</span></span><br><span class="line"><span class="params">        learning_rate: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        initial_epsilon: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        epsilon_decay: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        final_epsilon: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        discount_factor: <span class="built_in">float</span> = <span class="number">0.95</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initialize a Reinforcement Learning agent with an empty dictionary</span></span><br><span class="line"><span class="string">        of state-action values (q_values), a learning rate and an epsilon.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            env: The training environment</span></span><br><span class="line"><span class="string">            learning_rate: The learning rate</span></span><br><span class="line"><span class="string">            initial_epsilon: The initial epsilon value</span></span><br><span class="line"><span class="string">            epsilon_decay: The decay for epsilon</span></span><br><span class="line"><span class="string">            final_epsilon: The final epsilon value</span></span><br><span class="line"><span class="string">            discount_factor: The discount factor for computing the Q-value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.env = env</span><br><span class="line">        self.q_values = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.discount_factor = discount_factor</span><br><span class="line"></span><br><span class="line">        self.epsilon = initial_epsilon</span><br><span class="line">        self.epsilon_decay = epsilon_decay</span><br><span class="line">        self.final_epsilon = final_epsilon</span><br><span class="line"></span><br><span class="line">        self.training_error = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_action</span>(<span class="params">self, obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns the best action with probability (1 - epsilon)</span></span><br><span class="line"><span class="string">        otherwise a random action with probability epsilon to ensure exploration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># with probability epsilon return a random action to explore the environment</span></span><br><span class="line">        <span class="keyword">if</span> np.random.random() &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> self.env.action_space.sample()</span><br><span class="line">        <span class="comment"># with probability (1 - epsilon) act greedily (exploit)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(np.argmax(self.q_values[obs]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>],</span></span><br><span class="line"><span class="params">        action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        terminated: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">        next_obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Updates the Q-value of an action.&quot;&quot;&quot;</span></span><br><span class="line">        future_q_value = (<span class="keyword">not</span> terminated) * np.<span class="built_in">max</span>(self.q_values[next_obs])</span><br><span class="line">        temporal_difference = (</span><br><span class="line">            reward + self.discount_factor * future_q_value - self.q_values[obs][action]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.q_values[obs][action] = (</span><br><span class="line">            self.q_values[obs][action] + self.lr * temporal_difference</span><br><span class="line">        )</span><br><span class="line">        self.training_error.append(temporal_difference)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decay_epsilon</span>(<span class="params">self</span>):</span><br><span class="line">        self.epsilon = <span class="built_in">max</span>(self.final_epsilon, self.epsilon - self.epsilon_decay)</span><br><span class="line">   </span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>测试结果图</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://gymnasium.org.cn/introduction/train_agent/" >https://gymnasium.org.cn/introduction/train_agent/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<ol>
<li><p>二十一点规则</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://gymnasium.org.cn/environments/toy_text/blackjack/" >https://gymnasium.org.cn/environments/toy_text/blackjack/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</li>
<li><p>具体实现</p>
<ul>
<li><p>训练代码</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> BlackjackAgent <span class="keyword">import</span> BlackjackAgent  <span class="comment"># 导入 BlackjackAgent 类</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">n_episodes = <span class="number">100_000</span></span><br><span class="line">start_epsilon = <span class="number">1.0</span></span><br><span class="line">epsilon_decay = start_epsilon / (n_episodes / <span class="number">2</span>)  <span class="comment"># 逐步减少探索</span></span><br><span class="line">final_epsilon = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建环境</span></span><br><span class="line">env = gym.make(<span class="string">&quot;Blackjack-v1&quot;</span>, sab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化代理</span></span><br><span class="line">agent = BlackjackAgent(</span><br><span class="line">    env=env,</span><br><span class="line">    learning_rate=learning_rate,</span><br><span class="line">    initial_epsilon=start_epsilon,</span><br><span class="line">    epsilon_decay=epsilon_decay,</span><br><span class="line">    final_epsilon=final_epsilon,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">env = gym.wrappers.RecordEpisodeStatistics(env)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于记录奖励和误差</span></span><br><span class="line">episode_rewards = []</span><br><span class="line">episode_errors = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_episodes)):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        action = agent.get_action(obs)</span><br><span class="line">        next_obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新代理</span></span><br><span class="line">        agent.update(obs, action, reward, terminated, next_obs)</span><br><span class="line"></span><br><span class="line">        total_reward += reward</span><br><span class="line">        done = terminated <span class="keyword">or</span> truncated</span><br><span class="line">        obs = next_obs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 记录每回合的奖励和误差</span></span><br><span class="line">    episode_rewards.append(total_reward)</span><br><span class="line">    episode_errors.append(np.mean(agent.training_error))  <span class="comment"># 记录当前回合的平均误差</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新 epsilon</span></span><br><span class="line">    agent.decay_epsilon()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练完成，绘制奖励和误差曲线</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制奖励曲线</span></span><br><span class="line">ax1.plot(np.convolve(episode_rewards, np.ones(<span class="number">100</span>)/<span class="number">100</span>, mode=<span class="string">&#x27;valid&#x27;</span>), label=<span class="string">&quot;Total Reward&quot;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&quot;Total Reward Over Episodes&quot;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&quot;Reward&quot;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 TD 误差曲线</span></span><br><span class="line">ax2.plot(episode_errors, label=<span class="string">&quot;TD Error&quot;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&quot;TD Error Over Episodes&quot;</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&quot;TD Error&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================== 方法二：测试智能体性能 ================== #</span></span><br><span class="line">test_episodes = <span class="number">1000</span>  <span class="comment"># 运行 1000 回合</span></span><br><span class="line">total_wins = <span class="number">0</span></span><br><span class="line">total_games = <span class="number">0</span></span><br><span class="line">total_rewards = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(test_episodes):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        action = np.argmax(agent.q_values[obs])  <span class="comment"># 选择最优策略</span></span><br><span class="line">        obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line">        total_reward += reward</span><br><span class="line">        done = terminated <span class="keyword">or</span> truncated</span><br><span class="line"></span><br><span class="line">    total_rewards.append(total_reward)</span><br><span class="line">    <span class="keyword">if</span> total_reward &gt; <span class="number">0</span>:</span><br><span class="line">        total_wins += <span class="number">1</span></span><br><span class="line">    total_games += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">win_rate = total_wins / total_games * <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Win rate over <span class="subst">&#123;test_episodes&#125;</span> episodes: <span class="subst">&#123;win_rate:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制测试奖励分布直方图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.hist(total_rewards, bins=<span class="number">20</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Total Reward&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Frequency&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test Reward Distribution&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>agent类代码</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BlackjackAgent.py</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BlackjackAgent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        env: gym.Env,</span></span><br><span class="line"><span class="params">        learning_rate: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        initial_epsilon: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        epsilon_decay: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        final_epsilon: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        discount_factor: <span class="built_in">float</span> = <span class="number">0.95</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initialize a Reinforcement Learning agent with an empty dictionary</span></span><br><span class="line"><span class="string">        of state-action values (q_values), a learning rate and an epsilon.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            env: The training environment</span></span><br><span class="line"><span class="string">            learning_rate: The learning rate</span></span><br><span class="line"><span class="string">            initial_epsilon: The initial epsilon value</span></span><br><span class="line"><span class="string">            epsilon_decay: The decay for epsilon</span></span><br><span class="line"><span class="string">            final_epsilon: The final epsilon value</span></span><br><span class="line"><span class="string">            discount_factor: The discount factor for computing the Q-value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.env = env</span><br><span class="line">        self.q_values = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.discount_factor = discount_factor</span><br><span class="line"></span><br><span class="line">        self.epsilon = initial_epsilon</span><br><span class="line">        self.epsilon_decay = epsilon_decay</span><br><span class="line">        self.final_epsilon = final_epsilon</span><br><span class="line"></span><br><span class="line">        self.training_error = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_action</span>(<span class="params">self, obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns the best action with probability (1 - epsilon)</span></span><br><span class="line"><span class="string">        otherwise a random action with probability epsilon to ensure exploration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># with probability epsilon return a random action to explore the environment</span></span><br><span class="line">        <span class="keyword">if</span> np.random.random() &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> self.env.action_space.sample()</span><br><span class="line">        <span class="comment"># with probability (1 - epsilon) act greedily (exploit)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(np.argmax(self.q_values[obs]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>],</span></span><br><span class="line"><span class="params">        action: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        reward: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        terminated: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">        next_obs: <span class="built_in">tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">bool</span>],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Updates the Q-value of an action.&quot;&quot;&quot;</span></span><br><span class="line">        future_q_value = (<span class="keyword">not</span> terminated) * np.<span class="built_in">max</span>(self.q_values[next_obs])</span><br><span class="line">        temporal_difference = (</span><br><span class="line">            reward + self.discount_factor * future_q_value - self.q_values[obs][action]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.q_values[obs][action] = (</span><br><span class="line">            self.q_values[obs][action] + self.lr * temporal_difference</span><br><span class="line">        )</span><br><span class="line">        self.training_error.append(temporal_difference)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decay_epsilon</span>(<span class="params">self</span>):</span><br><span class="line">        self.epsilon = <span class="built_in">max</span>(self.final_epsilon, self.epsilon - self.epsilon_decay)</span><br><span class="line">   </span><br></pre></td></tr></table></figure></div>
</li>
<li><p>测试结果图</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071006683.png"
                      alt="v"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071006459.png"
                     
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/POCRO/myPic/pics/202502071007589.png"
                     
                ></p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> Q-learning</li>
        <li><strong>Author:</strong> POCRO</li>
        <li><strong>Created at
                :</strong> 2025-02-07 09:46:33</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-02-07 10:07:13
            </li>
        
        <li>
            <strong>Link:</strong> https://pocro.github.io/2025/02/07/Q-learning/
        </li>
        <li>
            <strong>
                License:
            </strong>
            
            This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
            

        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/2025/06/08/Notes%20for%20Control%20Design%20example%20for%20antenna/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item"></span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2025/02/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">强化学习基本概念</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container">
                <div class="comments-container pjax">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-swup-reload-script>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">Q-learning</div>
        <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E5%8D%9A%E5%AE%A2%E4%B8%8E%E8%AE%BA%E6%96%87"><span class="nav-text">参考博客与论文</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">1.基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%85%B3%E4%BA%8EOn-policy%E5%92%8COff-policy"><span class="nav-text">2. 关于On-policy和Off-policy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-On-policy%EF%BC%88%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A1%8C%E4%B8%BA%E9%80%89%E6%8B%A9%E4%B8%80%E8%87%B4%EF%BC%89%EF%BC%9A"><span class="nav-text">1. On-policy（策略学习与行为选择一致）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94%EF%BC%9A"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A"><span class="nav-text">例子：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Off-policy%EF%BC%88%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A1%8C%E4%B8%BA%E9%80%89%E6%8B%A9%E4%B8%8D%E4%B8%80%E8%87%B4%EF%BC%89%EF%BC%9A"><span class="nav-text">2. Off-policy（策略学习与行为选择不一致）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94%EF%BC%9A-1"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A-1"><span class="nav-text">例子：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-learning-%E6%98%AF-Off-policy%EF%BC%9A"><span class="nav-text">Q-learning 是 Off-policy：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Q-Learning-demo-for-Blackjack-in-Gymnasium%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%E7%82%B9%EF%BC%89"><span class="nav-text">3. Q-Learning demo for Blackjack in Gymnasium（二十一点）</span></a>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2023</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">POCRO</a>
        </div>
        
            <script data-swup-reload-script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.4.4</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex justify-center items-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
        ],
        containers: ["#swup"],
    });

    swup.hooks.on("page:view", () => {
        Global.refresh();
    });

    // if (document.readyState === "complete") {
    //
    // } else {
    //     document.addEventListener("DOMContentLoaded", () => init());
    // }
</script>






<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
